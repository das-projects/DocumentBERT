% bibliography

@article{ankner2023dynamic,
  title={Dynamic Masking Rate Schedules for MLM Pretraining},
  author={Ankner, Zachary and Saphra, Naomi and Blalock, Davis and Frankle, Jonathan and Leavitt, Matthew L},
  journal={arXiv preprint arXiv:2305.15096},
  year={2023}
}

@article{bai2021pre,
  title={Pre-train or annotate? domain adaptation with a constrained budget},
  author={Bai, Fan and Ritter, Alan and Xu, Wei},
  journal={arXiv preprint arXiv:2109.04711},
  year={2021}
}

@article{beltagy2019scibert,
  title={SciBERT: A pretrained language model for scientific text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  journal={arXiv preprint arXiv:1903.10676},
  year={2019}
}

% RTE
@inproceedings{bentivogli2009fifth,
  title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  booktitle={TAC},
  year={2009},
  organization={Citeseer}
}


% NO PROPER CITATION FOR BIOMEDLM???
@misc{bioMedLM,
  title = {BioMedLM},
  author = {Bolton, Elliot and Hall, David and Yasunaga, Michihiro and Lee, Tony and Manning, Chris and Liang, Percy},
  howpublished = {\url{https://crfm.stanford.edu/2022/12/15/biomedlm.html}},
  year = {2022},
  month = {December},
  note = {Accessed on 15th May 2023}
}

@article{blalock2020state,
  title={What is the state of neural network pruning?},
  author={Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
  journal={Proceedings of machine learning and systems},
  volume={2},
  pages={129--146},
  year={2020}
}

@article{blakeney2022reduce,
  title={Reduce, Reuse, Recycle: Improving Training Efficiency with Distillation},
  author={Blakeney, Cody and Forde, Jessica Zosa and Frankle, Jonathan and Zong, Ziliang and Leavitt, Matthew L.},
  journal={arXiv preprint arXiv:2211.00683},
  year={2022}
}

@article{brock2017freezeout,
  title={Freezeout: Accelerate training by progressively freezing layers},
  author={Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
  journal={arXiv preprint arXiv:1706.04983},
  year={2017}
}

% STSB
@article{cer2017semeval,
  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  journal={arXiv preprint arXiv:1708.00055},
  year={2017}
}

% Interesting, slightly random
@article{chiang2020pre,
  title={Pre-training a language model without human language},
  author={Chiang, Cheng-Han and Lee, Hung-yi},
  journal={arXiv preprint arXiv:2012.11995},
  year={2020}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}


@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

% One of many RTE references
@inproceedings{dagan2006pascal,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers},
  pages={177--190},
  year={2006},
  organization={Springer}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR}
}

@article{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  journal={arXiv preprint arXiv:2302.05442},
  year={2023}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{dey2023btlm,
  title={BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model},
  author={Dey, Nolan and Soboleva, Daria and Al-Khateeb, Faisal and Yang, Bowen and Pathria, Ribhu and Khachane, Hemant and Muhammad, Shaheer and Myers, Robert and Steeves, Jacob Robert and Vassilieva, Natalia and others},
  journal={arXiv preprint arXiv:2309.11568},
  year={2023}
}

% MRPC
@inproceedings{dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, Bill and Brockett, Chris},
  booktitle={Third International Workshop on Paraphrasing (IWP2005)},
  year={2005}
}

% Domain specific BERT
@inproceedings{el2022re,
  title={Re-train or train from scratch? Comparing pre-training strategies of BERT in the medical domain},
  author={El Boukkouri, Hicham and Ferret, Olivier and Lavergne, Thomas and Zweigenbaum, Pierre},
  booktitle={LREC 2022},
  pages={2626--2633},
  year={2022}
}

@article{frankle2020early,
  title={The early phase of neural network training},
  author={Frankle, Jonathan and Schwab, David J and Morcos, Ari S},
  journal={arXiv preprint arXiv:2002.10365},
  year={2020}
}

@inproceedings{geiping2023cramming,
  title={Cramming: Training a Language Model on a single GPU in one day.},
  author={Geiping, Jonas and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={11117--11143},
  year={2023},
  organization={PMLR}
}

% One of many RTE references
@inproceedings{giampiccolo2007third,
  title={The third pascal recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, William B},
  booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
  pages={1--9},
  year={2007}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	url={http://Skylion007.github.io/OpenWebTextCorpus}, 
	year={2019}
}

@article{golatkar2019time,
  title={Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence},
  author={Golatkar, Aditya Sharad and Achille, Alessandro and Soatto, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

% layer stacking
@inproceedings{gong2019efficient,
  title={Efficient training of BERT by progressively stacking},
  author={Gong, Linyuan and He, Di and Li, Zhuohan and Qin, Tao and Wang, Liwei and Liu, Tieyan},
  booktitle={International conference on machine learning},
  pages={2337--2346},
  year={2019},
  organization={PMLR}
}

@inproceedings{goujaud2022super,
  title={Super-acceleration with cyclical step-sizes},
  author={Goujaud, Baptiste and Scieur, Damien and Dieuleveut, Aymeric and Taylor, Adrien B and Pedregosa, Fabian},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3028--3065},
  year={2022},
  organization={PMLR}
}

@article{gu2021domain,
  title={Domain-specific language model pretraining for biomedical natural language processing},
  author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  volume={3},
  number={1},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY}
}

% Domain Specific
@article{gururangan2020don,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

% An ELECTRA Style Model
@article{he2021debertav3,
  title={Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing},
  author={He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2111.09543},
  year={2021}
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{henry2020query,
  title={Query-key normalization for transformers},
  author={Henry, Alex and Dachapally, Prudhvi Raj and Pawar, Shubham and Chen, Yuxuan},
  journal={arXiv preprint arXiv:2010.04245},
  year={2020}
}

% Domain Specific BERT
@inproceedings{horawalavithana2022foundation,
  title={Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned},
  author={Horawalavithana, Sameera and Ayton, Ellyn and Sharma, Shivam and Howland, Scott and Subramanian, Megha and Vasquez, Scott and Cosbey, Robin and Glenski, Maria and Volkova, Svitlana},
  booktitle={Proceedings of BigScience Episode$\backslash$\# 5--Workshop on Challenges \& Perspectives in Creating Large Language Models},
  pages={160--172},
  year={2022}
}

@article{howard2020fastai,
  title={Fastai: a layered API for deep learning},
  author={Howard, Jeremy and Gugger, Sylvain},
  journal={Information},
  volume={11},
  number={2},
  pages={108},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@inproceedings{hua2022transformer,
  title={Transformer quality in linear time},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  booktitle={International Conference on Machine Learning},
  pages={9099--9117},
  year={2022},
  organization={PMLR}
}

@article{huang2017snapshot,
  title={Snapshot ensembles: Train 1, get m for free},
  author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:1704.00109},
  year={2017}
}

% Related to RAG
@article{izacard2022unsupervised,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  year={2022}
}

@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

@article{izsak2021train,
  title={How to train BERT with an academic budget},
  author={Izsak, Peter and Berchansky, Moshe and Levy, Omer},
  journal={arXiv preprint arXiv:2104.07705},
  year={2021}
}

% QQP source
@online{Iyer2017Quora,
  author = {Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornel},
  title = {First Quora Dataset Release: Question Pairs},
  year = 2017,
  url = {https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs},
  urldate = {2019-04-03}
}

% Related to RAG models
@article{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}

@article{kaddour2023no,
  title={No train no gain: Revisiting efficient training algorithms for transformer-based language models},
  author={Kaddour, Jean and Key, Oscar and Nawrot, Piotr and Minervini, Pasquale and Kusner, Matt J},
  journal={arXiv preprint arXiv:2307.06440},
  year={2023}
}

@article{komatsuzaki2019one,
  title={One epoch is all you need},
  author={Komatsuzaki, Aran},
  journal={arXiv preprint arXiv:1906.06669},
  year={2019}
}

@article{korthikanti2022reducing,
  title={Reducing activation recomputation in large transformer models},
  author={Korthikanti, Vijay and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2205.05198},
  year={2022}
}

% Domain Specific BERT
@article{krishna2022downstream,
  title={Downstream datasets make surprisingly good pretraining corpora},
  author={Krishna, Kundan and Garg, Saurabh and Bigham, Jeffrey P and Lipton, Zachary C},
  journal={arXiv preprint arXiv:2209.14389},
  year={2022}
}

% AlBERT, which uses weight sharing
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

% Domain Specific BERT
@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}



%WNLI
@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012}
}


% RAG
@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{li2023narrowbert,
  title={NarrowBERT: Accelerating Masked Language Model Pretraining and Inference},
  author={Li, Haoxin and Keung, Phillip and Cheng, Daniel and Kasai, Jungo and Smith, Noah A},
  journal={arXiv preprint arXiv:2301.04761},
  year={2023}
}

@article{lin2020controllable,
  title={Controllable pareto multi-task learning},
  author={Lin, Xi and Yang, Zhiyuan and Zhang, Qingfu and Kwong, Sam},
  journal={arXiv preprint arXiv:2010.06313},
  year={2020}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

% MISCELLANEOUS ARTICLE ABOUT BIAS IN PRETRAINING VS FINETUNING
@article{liu2022same,
  title={Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models},
  author={Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu},
  journal={arXiv preprint arXiv:2210.14199},
  year={2022}
}

% Mosaicml MPT-30B reference
@misc{MosaicML2023IntroducingMPT30B,
	title        = {Introducing MPT-30B: Raising the bar for open-source foundation models},
	author       = {{MosaicML NLP Team}},
	year         = 2023,
	urldate      = {2023-06-22},
	note         = {Accessed: 2023-06-22},
	howpublished = {\url{www.mosaicml.com/blog/mpt-30b}}
}
% Mosaicml MPT-7B reference
@misc{MosaicML2023IntroducingMPT7B,
	title        = {Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs},
	author       = {{MosaicML NLP Team}},
	year         = 2023,
	urldate      = {2023-05-05},
	note         = {Accessed: 2023-05-05},
	howpublished = {\url{www.mosaicml.com/blog/mpt-7b}}
}

@misc{leavittmosaicml2022,
	title = {{MosaicML} {ResNet} {Deep} {Dive}},
	url = {https://www.mosaicml.com/blog/mosaic-resnet-deep-dive},
	abstract = {TL;DR: We recently released a set of recipes which can accelerate training of a ResNet-50 on ImageNet by up to 7x over standard baselines. In this report we take a deep dive into the technical details of our work and share the insights we gained about optimizing the efficiency of model training over a broad range of compute budgets.},
	urldate = {2022-11-11},
	author = {Leavitt, Matthew},
	year = {2022},
	file = {Snapshot:/Users/mleavitt/Zotero/storage/VM2772BR/mosaic-resnet-deep-dive.html:text/html},
}

@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{muller2019does,
  title={When does label smoothing help?},
  author={M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

% Common Crawl News Dataset
@misc{nagel2016common,
  author = {Nagel, Sebastian},
  title = {Common Crawl Blog - News Dataset Available},
  year = {2023},
  url = {https://commoncrawl.org/blog/news-dataset-available},
  note = {Accessed: 2016-10-4}
}


@article{navon2020learning,
  title={Learning the pareto front with hypernetworks},
  author={Navon, Aviv and Shamsian, Aviv and Chechik, Gal and Fetaya, Ethan},
  journal={arXiv preprint arXiv:2010.04104},
  year={2020}
}

@article{narang2021transformer,
  title={Do transformer modifications transfer across implementations and applications?},
  author={Narang, Sharan and Chung, Hyung Won and Tay, Yi and Fedus, William and Fevry, Thibault and Matena, Michael and Malkan, Karishma and Fiedel, Noah and Shazeer, Noam and Lan, Zhenzhong and others},
  journal={arXiv preprint arXiv:2102.11972},
  year={2021}
}

@misc{portes2022efficiently,
	title = {Efficiently Estimating Pareto Frontiers with Cyclic Learning Rate Schedules},
	url = {https://www.mosaicml.com/blog/efficiently-estimating-pareto-frontiers},
	urldate = {2022-11-11},
	author = {Portes, Jacob},
	year = {2022},
}

@misc{mosaicml2022methodology,
	title = {Methodology},
	url = {https://www.mosaicml.com/blog/methodology},
	urldate = {2022-11-11},
	author = {{MosaicML Research Team}},
	year = {2022},

}

@misc{mosaicml2022mlperf,
	title = {MosaicML Delivers Leading NLP Performance in MLPerf v2.1},
	url = {https://www.mosaicml.com/blog/mlperf-nlp-nov2022},
	urldate = {2022-11-11},
	author = {Khudia, Daya and Sardana, Nikhil and Havens, Sam and Trott, Alex and Yuen, Erica Ji},
	year = {2022},
}

@article{portes2022fast,
  title={Fast Benchmarking of Accuracy vs. Training Time with Cyclic Learning Rates},
  author={Portes, Jacob and Blalock, Davis and Stephenson, Cory and Frankle, Jonathan},
  journal={arXiv preprint arXiv:2206.00832},
  year={2022}
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

% T5 and C4 paper
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

% Naomi is on this paper
@article{sellam2021multiberts,
  title={The multiberts: Bert reproductions for robustness analysis},
  author={Sellam, Thibault and Yadlowsky, Steve and Wei, Jason and Saphra, Naomi and D'Amour, Alexander and Linzen, Tal and Bastings, Jasmijn and Turc, Iulia and Eisenstein, Jacob and Das, Dipanjan and others},
  journal={arXiv preprint arXiv:2106.16163},
  year={2021}
}

@inproceedings{schmidt2021descending,
  title={Descending through a crowded valley-benchmarking deep learning optimizers},
  author={Schmidt, Robin M and Schneider, Frank and Hennig, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={9367--9376},
  year={2021},
  organization={PMLR}
}

% Domain Specific BERT
@article{shah2022flue,
  title={WHEN FLUE MEETS FLANG: Benchmarks and Large Pre-trained Language Model for Financial Domain},
  author={Shah, Raj Sanjay and Chawla, Kunal and Eidnani, Dheeraj and Shah, Agam and Du, Wendi and Chava, Sudheer and Raman, Natraj and Smiley, Charese and Chen, Jiaao and Yang, Diyi},
  journal={arXiv preprint arXiv:2211.00083},
  year={2022}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

% Domain Specific BERT
@article{shen2021mathbert,
  title={Mathbert: A pre-trained language model for general nlp tasks in mathematics education},
  author={Shen, Jia Tracy and Yamashita, Michiharu and Prihar, Ethan and Heffernan, Neil and Wu, Xintao and Graff, Ben and Lee, Dongwon},
  journal={arXiv preprint arXiv:2106.07340},
  year={2021}
}

% RoPE
@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

% performance of downstream tasks vs. loss  do not necessarily correlate
@article{tay2021scale,
  title={Scale efficiently: Insights from pre-training and fine-tuning transformers},
  author={Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus, William and Abnar, Samira and Chung, Hyung Won and Narang, Sharan and Yogatama, Dani and Vaswani, Ashish and Metzler, Donald},
  journal={arXiv preprint arXiv:2109.10686},
  year={2021}
}

% Stories dataset used in RoBERTa
@article{trinh2018simple,
  title={A simple method for commonsense reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}

@article{rabe2021self,
  title={Self-attention Does Not Need $O(n^{2})$ Memory},
  author={Rabe, Markus N and Staats, Charles},
  journal={arXiv preprint arXiv:2112.05682},
  year={2021}
}

% Apparently this is QNLI
@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{rodgers1985improvements,
  title={Improvements in multiprocessor system design},
  author={Rodgers, David P},
  journal={ACM SIGARCH Computer Architecture News},
  volume={13},
  number={3},
  pages={225--231},
  year={1985},
  publisher={ACM New York, NY, USA}
}

% Related to RAG
@article{shi2023replug,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{smith2017cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N},
  booktitle={2017 IEEE winter conference on applications of computer vision (WACV)},
  pages={464--472},
  year={2017},
  organization={IEEE}
}

@article{smith2022general,
  title={General Cyclical Training of Neural Networks},
  author={Smith, Leslie N},
  journal={arXiv preprint arXiv:2202.08835},
  year={2022}
}

% Original SST (Stanford Sentiment Tree Bank) paper
@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

% Domain specific BERT
@article{tabassum2020code,
  title={Code and named entity recognition in stackoverflow},
  author={Tabassum, Jeniya and Maddela, Mounica and Xu, Wei and Ritter, Alan},
  journal={arXiv preprint arXiv:2005.01634},
  year={2020}
}

@inproceedings{tillet2019triton,
  title={Triton: an intermediate language and compiler for tiled neural network computations},
  author={Tillet, Philippe and Kung, Hsiang-Tsung and Cox, David},
  booktitle={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={10--19},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% GLUE paper!
@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

% CoLA Paper
@article{warstadt2019neural,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={625--641},
  year={2019},
  publisher={MIT Press}
}

@article{wettig2022should,
  title={Should you mask 15\% in masked language modeling?},
  author={Wettig, Alexander and Gao, Tianyu and Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2202.08005},
  year={2022}
}

% MNLI
@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@inproceedings{wu2019demystifying,
  title={Demystifying learning rate policies for high accuracy training of deep neural networks},
  author={Wu, Yanzhao and Liu, Ling and Bae, Juhyun and Chow, Ka-Ho and Iyengar, Arun and Pu, Calton and Wei, Wenqi and Yu, Lei and Zhang, Qi},
  booktitle={2019 IEEE International conference on big data (Big Data)},
  pages={1971--1980},
  year={2019},
  organization={IEEE}
}


% Domain Specific BERT
@article{wu2022insights,
  title={Insights into pre-training via simpler synthetic tasks},
  author={Wu, Yuhuai and Li, Felix and Liang, Percy S},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21844--21857},
  year={2022}
}

% Domain Specific BERT
@article{yasunaga2022linkbert,
  title={Linkbert: Pretraining language models with document links},
  author={Yasunaga, Michihiro and Leskovec, Jure and Liang, Percy},
  journal={arXiv preprint arXiv:2203.15827},
  year={2022}
}

@article{you2019does,
  title={How does learning rate decay help modern neural networks?},
  author={You, Kaichao and Long, Mingsheng and Wang, Jianmin and Jordan, Michael I},
  journal={arXiv preprint arXiv:1908.01878},
  year={2019}
}

& E5 paper
@article{wang2022text,
  title={Text embeddings by weakly-supervised contrastive pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}

% Bloomberg GPT!
@article{wu2023bloomberggpt,
  title={Bloomberggpt: A large language model for finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023}
}

@article{zeng2022boosting,
  title={Boosting Distributed Training Performance of the Unpadded BERT Model},
  author={Zeng, Jinle and Li, Min and Wu, Zhihua and Liu, Jiaqi and Liu, Yuang and Yu, Dianhai and Ma, Yanjun},
  journal={arXiv preprint arXiv:2208.08124},
  year={2022}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@inproceedings{zhang2019making,
  title={Making convolutional networks shift-invariant again},
  author={Zhang, Richard},
  booktitle={International conference on machine learning},
  pages={7324--7334},
  year={2019},
  organization={PMLR}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}



% Book Corpus
@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

